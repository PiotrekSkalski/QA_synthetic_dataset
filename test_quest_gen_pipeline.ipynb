{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597326983793",
   "display_name": "Python 3.7.7 64-bit ('torch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    BertTokenizer,\n",
    "    BertForQuestionAnswering\n",
    ")\n",
    "from utils import find_subsequences\n",
    "from question_generator_utils import SyntheticAnswersDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_questions(BERT_tokenizer, BERT_model, context, questions):\n",
    "    batch_contexts = [context] * len(questions)\n",
    "    inputs = BERT_tokenizer(batch_contexts, questions, padding=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        start_logits, end_logits = BERT_model(**inputs)[:2]\n",
    "    \n",
    "    token_type_ids = inputs['token_type_ids'].bool()\n",
    "    attention_mask = inputs['attention_mask'].bool()\n",
    "    start_logits.masked_fill_(torch.logical_or(token_type_ids, ~attention_mask), -float('inf'))\n",
    "    end_logits.masked_fill_(torch.logical_or(token_type_ids, ~attention_mask), -float('inf'))\n",
    "\n",
    "    is_answerable = ~ torch.logical_and(\n",
    "        (start_logits[:, 0].unsqueeze(-1) >= start_logits).all(dim=-1),\n",
    "        (end_logits[:, 0].unsqueeze(-1) >= end_logits).all(dim=-1)\n",
    "    )\n",
    "\n",
    "    start_probs = F.softmax(start_logits, dim=-1)\n",
    "    end_probs = F.softmax(end_logits, dim=-1)\n",
    "\n",
    "    ratings = start_probs[:, 0] * end_probs[:, 0]\n",
    "\n",
    "    return ratings, is_answerable, (start_logits, end_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "    'question_generation_model_q-loss',\n",
    "    do_lower_case=True\n",
    ")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\n",
    "    'question_generation_model_q-loss',\n",
    ")\n",
    "gpt2_model.eval();\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "    'deepset/bert-large-uncased-whole-word-masking-squad2',\n",
    "    do_lower_case=True\n",
    ")\n",
    "bert_model = BertForQuestionAnswering.from_pretrained(\n",
    "    'deepset/bert-large-uncased-whole-word-masking-squad2'\n",
    ")\n",
    "bert_model.eval();\n",
    "\n",
    "def prepare_inputs_for_generation(input_ids, past, **kwargs):\n",
    "    if past:\n",
    "        input_ids = input_ids[:, -1].unsqueeze(-1)\n",
    "        token_type_ids = kwargs['token_type_ids'][:, -1].unsqueeze(-1)\n",
    "    else:\n",
    "        token_type_ids = kwargs['token_type_ids']\n",
    "    return {'input_ids': input_ids, 'past': past, 'use_cache': kwargs['use_cache'],\n",
    "            'token_type_ids': token_type_ids}\n",
    "gpt2_model.prepare_inputs_for_generation = prepare_inputs_for_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = SyntheticAnswersDataset('generated_answers', gpt2_tokenizer)\n",
    "dl = DataLoader(\n",
    "    ds,\n",
    "    batch_size=1\n",
    ")\n",
    "dl = iter(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
    }
   ],
   "source": [
    "output_sequences = gpt2_model.generate(\n",
    "            input_ids=batch[1],\n",
    "            token_type_ids=batch[2],\n",
    "            max_length=64 + batch[1].shape[-1],\n",
    "            temperature=1.0,\n",
    "            top_k=30,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=5\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "5"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "len(output_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_start_token = torch.tensor(gpt2_tokenizer.encode('question:'),dtype=torch.long)\n",
    "q_end_token = torch.tensor(gpt2_tokenizer.encode(':question'), dtype=torch.long)\n",
    "\n",
    "# Decode questions\n",
    "questions = []\n",
    "for output in output_sequences:\n",
    "    q_start_idx = (output.squeeze() == q_start_token).nonzero()[0]\n",
    "    q_end_indices = (output.squeeze() == q_end_token).nonzero()\n",
    "    if len(q_end_indices):\n",
    "        q_end_idx = q_end_indices[0]\n",
    "    else:\n",
    "        continue\n",
    "    question = gpt2_tokenizer.decode(output[q_start_idx + 1: q_end_idx], clean_up_tokenization_spaces=True)\n",
    "    questions.append(question.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['who replaced giovanni studdani after his injury?',\n 'what position did doni hold at miafa?',\n 'who did ku hire in the summer of 2012?',\n 'how long was captain glover alderwood suspended for?',\n 'who was named player of the year in 2012?']"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = batch[0][0]\n",
    "ratings, is_answerable, logits= rate_questions(bert_tokenizer, bert_model, context, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([False, False, False, False, False])"
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "is_answerable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 5.7868, -3.7926, -6.0274, -7.4328, -8.3713, -5.6004, -2.0553, -7.4835,\n         -6.3547, -7.3850, -7.6516, -5.3609, -3.0608, -5.8211, -4.3033, -1.8704,\n         -5.4540, -1.6528, -4.8081, -5.6938, -4.6707, -4.4648, -6.1712, -6.9896,\n         -5.1679, -5.2939, -5.4064, -2.8466, -6.1642, -6.1090, -5.5700, -6.2520,\n         -5.5366, -3.8833, -6.5645, -5.0065, -4.8451, -6.9143, -5.6570, -6.1770,\n         -5.3950, -5.9186, -4.3723, -5.7299, -3.5920, -4.8077, -3.3176, -2.9515,\n         -5.7393, -5.3693, -3.2375, -7.1916, -5.8638, -4.7915, -7.4790, -6.7465,\n         -6.6031, -5.9000, -6.9159, -7.4453, -5.6511, -5.4522, -4.6808, -6.2935,\n         -3.9609, -4.2125, -5.7507, -6.7583, -5.6914, -4.2513, -6.1823, -2.2694,\n         -6.6377, -6.4860, -6.8543, -7.5613, -6.8649, -6.5271, -7.5793, -7.7510,\n         -7.2026, -6.8518, -8.2672, -6.5676, -6.7495, -7.9915, -6.5225, -6.6077,\n         -7.6720, -5.8631, -6.3905, -4.1636, -5.5168, -7.2593, -6.3871, -5.2770,\n         -6.8056, -7.2830, -5.9771, -6.2919, -7.1390, -7.9693, -8.5278, -7.4518,\n         -5.6976, -6.4975, -6.3110, -1.7915, -6.2875, -5.5299, -7.3396,  0.0610,\n         -6.1491, -7.2659, -6.8539, -5.5467, -6.3170, -3.3158, -6.7665, -4.1585,\n         -3.8982, -5.7150, -5.2277, -5.2285, -5.3236, -2.1613, -3.8360, -4.7851,\n         -3.1200, -6.8241, -3.1512, -5.3713, -4.3455, -6.9055, -6.1053, -5.4676,\n         -6.5100, -3.1993, -1.2643, -6.2758, -7.4057, -7.8112, -5.5419, -5.5302,\n         -4.4081, -4.7429, -6.2466, -2.3758, -6.9671, -7.4636, -3.6767, -6.9186,\n         -6.9984, -4.8586, -5.4558, -5.4945, -3.7205, -7.8069, -7.5327, -8.3005,\n         -8.6957, -6.8646, -5.5550, -3.2950, -7.4170, -8.2284, -5.8682, -5.1510,\n         -3.1646, -7.1857, -7.0902, -7.5774, -8.0570, -4.5112, -7.6169, -5.1139,\n         -5.5337, -8.4916, -7.8222, -2.7396, -2.5757, -7.0300, -8.1083, -7.2880,\n         -7.2614, -7.2761, -9.5690, -3.8282,    -inf,    -inf,    -inf,    -inf,\n            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n        [ 5.5688, -3.7606, -6.3826, -7.8682, -8.6125, -5.5501, -2.4367, -7.8946,\n         -6.7138, -7.6797, -7.9737, -5.6350, -3.2652, -5.9704, -4.7782, -2.3312,\n         -5.6624, -1.9104, -4.9180, -6.0602, -5.0252, -5.3837, -6.4930, -7.3106,\n         -5.5257, -5.6976, -5.5950, -3.3289, -6.4934, -6.5485, -5.9927, -6.8036,\n         -5.8254, -4.1819, -7.0316, -5.5543, -5.4323, -7.3427, -6.3422, -6.6544,\n         -5.8614, -6.3290, -4.6279, -5.9260, -3.9943, -5.0800, -3.5402, -3.2017,\n         -5.9326, -5.6475, -3.9208, -7.6344, -6.3999, -5.6208, -7.8994, -7.5249,\n         -7.1588, -6.8090, -7.3163, -7.9674, -6.4607, -6.2320, -5.3864, -6.8553,\n         -4.6960, -5.2160, -6.1562, -7.1893, -6.5038, -5.4966, -7.5494, -4.9451,\n         -7.4550, -6.8670, -7.8837, -7.8359, -7.1947, -6.8789, -7.9878, -8.2851,\n         -7.4369, -7.6156, -8.6127, -7.4007, -7.5953, -8.5374, -7.1058, -7.2749,\n         -7.9295, -7.1604, -7.4348, -5.8469, -6.3466, -7.4559, -6.9930, -5.6239,\n         -7.5675, -7.7466, -6.5540, -6.7687, -7.5869, -8.2570, -8.8431, -8.1932,\n         -6.9874, -7.2495, -6.9110, -3.6917, -7.1925, -6.7265, -7.9889, -0.4519,\n         -6.6085, -7.3706, -6.8708, -7.1516, -7.1456, -3.9848, -8.0090, -5.9626,\n         -5.7138, -7.0182, -6.4188, -6.7938, -6.3735, -4.5118, -4.9114, -6.0734,\n         -4.2312, -7.7129, -4.0435, -6.9773, -4.9645, -8.1048, -7.2919, -6.9587,\n         -7.5751, -4.6811, -2.5203, -6.8418, -7.6753, -8.1823, -6.6913, -6.1374,\n         -5.2320, -6.1367, -7.2990, -3.2197, -7.7830, -8.1221, -4.4079, -7.7680,\n         -7.6763, -5.9871, -6.8037, -6.3677, -3.4889, -8.0899, -8.0832, -8.8519,\n         -9.2182, -7.5163, -4.4819, -0.1158, -7.7383, -8.5030, -6.6709, -5.5522,\n         -5.2324, -8.3434, -7.4725, -8.2812, -8.9008, -5.7492, -8.5557, -5.4078,\n         -4.5235, -8.8784, -8.0102, -3.9478, -4.5193, -7.5508, -7.9605, -7.5273,\n         -7.2643, -6.7715, -9.5953, -3.8894,    -inf,    -inf,    -inf,    -inf,\n            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n        [ 6.2527, -3.5355, -6.0875, -7.6848, -8.5154, -5.7765, -2.8854, -7.9605,\n         -6.7898, -7.7121, -8.0678, -5.6268, -3.4538, -6.2560, -4.4073, -1.9913,\n         -5.5143, -1.3017, -4.6531, -6.1214, -4.9519, -5.5040, -6.6637, -7.4374,\n         -5.2487, -5.7041, -5.7152, -3.6818, -6.5682, -6.6206, -6.0153, -6.8091,\n         -5.8071, -3.9784, -7.2076, -5.7110, -5.6111, -7.4319, -6.4654, -6.6895,\n         -6.0030, -6.5331, -4.9666, -6.1839, -3.9100, -5.2534, -3.6890, -3.2666,\n         -6.0759, -5.9453, -4.1776, -7.8634, -6.8413, -6.1619, -8.0664, -7.7244,\n         -7.4174, -6.9893, -7.6667, -8.2418, -6.6974, -6.3095, -5.3738, -6.9594,\n         -4.7200, -5.2317, -6.2245, -7.3155, -6.5737, -5.3560, -6.2870, -2.5970,\n         -6.9411, -6.7868, -7.1715, -8.2213, -7.4720, -7.0121, -8.0940, -8.3302,\n         -7.6395, -7.6623, -8.5801, -7.4896, -7.5712, -8.5452, -7.2160, -7.4639,\n         -8.0451, -7.0085, -6.8679, -4.3613, -6.0850, -7.6525, -7.0640, -5.9597,\n         -7.5302, -7.8622, -6.7852, -6.9959, -7.5810, -8.3842, -8.8352, -7.6713,\n         -5.5427, -6.9909, -7.1506, -4.0139, -7.3325, -7.0399, -8.0978, -0.7438,\n         -6.2852, -7.4653, -7.2382, -7.7854, -7.4461, -4.2287, -8.3734, -6.8098,\n         -6.4031, -7.5339, -6.7221, -7.6309, -6.7155, -3.7789, -3.6811, -4.2099,\n         -2.1701, -7.4631, -3.3674, -6.4095, -3.9701, -7.6112, -6.7567, -6.1316,\n         -6.5349, -2.9755, -1.9875, -6.3564, -7.2224, -7.7667, -6.2588, -5.9880,\n         -5.0166, -5.6559, -7.3820, -3.2275, -7.9272, -8.5331, -4.3901, -7.7748,\n         -7.7711, -5.8622, -6.6553, -6.1036, -2.7027, -7.7011, -7.8741, -8.6139,\n         -8.8980, -7.3099, -5.0524, -2.0068, -8.2809, -8.5924, -6.5085, -6.0308,\n         -3.6401, -7.7225, -8.1232, -8.1750, -8.8391, -5.8463, -8.4316, -6.9764,\n         -6.7548, -9.4266, -8.1651, -2.6832,  0.0424, -6.2173, -7.7999, -7.5529,\n         -7.7729, -7.7456, -9.5794, -4.2564,    -inf,    -inf,    -inf,    -inf,\n            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n        [ 5.7979, -3.5863, -6.0523, -7.5516, -8.5467, -5.6291, -2.4482, -7.7322,\n         -6.6626, -7.6743, -8.0583, -5.8050, -3.9288, -5.9902, -4.5153, -2.2673,\n         -5.7071, -1.8684, -4.7859, -5.6910, -4.3542, -3.9103, -6.5266, -7.1540,\n         -5.0895, -5.3054, -5.3066, -3.1030, -6.1109, -6.2943, -5.5407, -6.4486,\n         -5.6739, -4.0419, -6.7521, -5.2233, -5.0692, -7.0603, -6.0731, -6.3496,\n         -5.5697, -6.1777, -4.5154, -5.9453, -3.9235, -4.9631, -3.4992, -3.1967,\n         -5.7532, -5.5781, -3.5304, -7.3127, -6.0942, -5.4160, -7.6783, -6.6871,\n         -6.5638, -5.3287, -6.9026, -7.4916, -5.9731, -6.0362, -5.3493, -5.9859,\n         -4.0471, -4.7702, -5.7138, -6.8735, -6.0898, -5.2478, -7.6107, -5.2692,\n         -7.4884, -6.7363, -7.8149, -6.8455, -6.1168, -5.2617, -7.3708, -7.7563,\n         -7.2289, -7.0351, -8.3690, -7.1706, -7.4055, -8.2917, -6.8551, -6.9599,\n         -7.8769, -6.8792, -7.2732, -5.7955, -6.2429, -7.3699, -6.5932, -5.4937,\n         -7.2480, -7.4657, -6.0561, -6.3760, -7.2400, -8.0497, -8.5934, -8.0506,\n         -6.9772, -6.9820, -6.5173, -2.6528, -6.5583, -6.0080, -7.8021, -0.5381,\n         -6.3578, -7.3508, -7.1582, -6.7079, -6.9806, -4.1176, -7.5811, -5.0829,\n         -5.0350, -6.5903, -6.0837, -6.2987, -6.0023, -3.1721, -3.8918, -5.1862,\n         -3.5543, -7.3803, -4.0187, -6.3033, -5.1418, -7.5101, -6.6362, -6.0923,\n         -6.9284, -4.0543, -2.6527, -6.5581, -7.5200, -7.9300, -6.2766, -5.9308,\n         -4.6470, -5.5823, -6.9338, -3.1609, -7.2245, -7.9203, -4.5046, -7.2852,\n         -7.5843, -5.8049, -6.1395, -6.2164, -4.1403, -7.9611, -7.1263, -8.0775,\n         -8.5758, -6.8529, -5.2579, -4.3639, -6.9459, -8.2218, -5.9077, -5.3542,\n         -6.0927, -8.4893, -7.8951, -8.4362, -8.3675, -7.2431, -8.6674, -4.0485,\n         -3.7206, -7.8333, -8.0963, -4.5502, -7.0470, -8.4268, -8.5574, -7.3240,\n         -7.5858, -7.4995, -9.5450, -4.1940,    -inf,    -inf,    -inf,    -inf,\n            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n        [ 5.8709, -3.4917, -6.0769, -7.6668, -8.6406, -5.5420, -2.1346, -7.7830,\n         -6.5769, -7.6137, -7.9269, -5.5635, -3.1175, -5.8580, -4.7741, -2.2396,\n         -5.7148, -1.8482, -4.9436, -6.0494, -4.9854, -5.2468, -6.6482, -7.2436,\n         -5.5063, -5.7274, -5.5898, -3.2293, -6.3957, -6.4636, -5.8587, -6.5884,\n         -5.6049, -3.9415, -6.8790, -5.3409, -5.2555, -7.1719, -6.1992, -6.4364,\n         -5.7344, -6.1691, -4.5619, -5.8401, -3.7824, -4.8953, -3.3788, -3.1512,\n         -5.8651, -5.5229, -3.7524, -7.7564, -6.3990, -5.9335, -7.8959, -7.4785,\n         -7.1819, -6.7131, -7.3743, -7.9184, -6.4129, -6.1041, -5.2566, -6.7253,\n         -4.4770, -4.9923, -6.1524, -7.0640, -6.6528, -5.6515, -7.4523, -4.1768,\n         -7.3771, -7.1041, -7.6301, -7.8923, -7.2559, -6.8570, -7.9497, -8.2084,\n         -7.3576, -7.4171, -8.5703, -7.4041, -7.5750, -8.4885, -7.0612, -7.2417,\n         -7.8369, -6.9337, -6.9939, -4.3445, -5.8968, -7.3770, -6.8603, -5.9130,\n         -7.4505, -7.7066, -6.5545, -6.9182, -7.5485, -8.2035, -8.8057, -8.0849,\n         -6.4302, -7.0004, -6.7022, -3.5705, -7.0214, -6.6152, -7.9107,  0.5555,\n         -6.1216, -7.1299, -6.1687, -7.4739, -7.2240, -4.0967, -8.0905, -6.3311,\n         -5.9178, -7.1150, -6.4373, -7.0594, -6.1499, -3.9508, -4.7174, -5.9604,\n         -4.0079, -7.4608, -3.9277, -6.7513, -5.1919, -7.9672, -7.1860, -6.7922,\n         -7.5131, -4.3599, -0.8174, -6.2557, -7.3204, -7.6575, -6.8974, -6.2856,\n         -5.6412, -6.2264, -7.4164, -3.5711, -7.7737, -8.2097, -4.7069, -7.6978,\n         -7.7927, -6.3065, -6.8292, -6.4800, -5.0148, -8.5221, -8.2902, -8.8497,\n         -9.3227, -8.2342, -6.6644, -4.9483, -8.1069, -8.5501, -7.2472, -7.5218,\n         -5.4989, -8.6943, -8.3972, -8.7219, -9.1133, -6.6502, -8.7663, -6.9864,\n         -6.8643, -9.2736, -8.4644, -4.4290, -3.5527, -7.3973, -8.7442, -8.1137,\n         -8.4581, -8.1120, -9.7165, -4.3418,    -inf,    -inf,    -inf,    -inf,\n            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf]])"
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "source": [
    "logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generated_answers/answers_0.json', 'r') as f:\n",
    "    examples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'on 19 july 2011, he signed a new contract with liverpool and joined championship team hull city in a year - long loan move. he made his full debut for hull in a start of the season clash against recently relegated blackpool at the kc stadium. he received a knee injury in a 1 – 0 defeat at burnley on 31 december 2011 and was substituted by adriano basso on the 42nd minute mark, shortly after conceding a goal, scored by martin paterson, as a result of a defensive mix - up with jack hobbs. following the injury, gulacsi returned to liverpool for a scan on his knee. on 11 april 2012, liverpool contacted hull with a view to recalling gulacsi from his loan subject to premier league, football league and fa approval. liverpool were at that time suffering a goalkeeper crisis, with both pepe reina and doni serving suspensions, leaving brad jones as their only remaining senior'"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "examples[0]['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[{'answer': '1888', 'seq_num': 1},\n {'answer': 'yves bouvier', 'seq_num': 2},\n {'answer': 'yves bouvier', 'seq_num': 1},\n {'answer': '5 %', 'seq_num': 1},\n {'answer': '1. 2 million works of art, allegedly including around 1000',\n  'seq_num': 1}]"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "examples[0]['generated_answers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_encoded = torch.tensor(tokenizer.encode(examples[0]['context']), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_encoded = torch.tensor(tokenizer.encode(' 1888,'), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([49584,    11])"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "answer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "' 1888,'"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "tokenizer.decode(answer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([ 1169, 15587,   286,   262,  2030, 45813,   460,   307, 23246,   736,\n          284, 49584,    11,   475,   355,   340,  9902,   287,  2546,    11,\n          340,  8197,   262,   564,   250, 32191, 15421,   286,  1509,   747,\n        11754,   564,   251,    11,  1642,   340,   262,  9871,  6143,  6841,\n          329,   262,  3230,  9085,    13,  1864,   284,   281,  2708,   287,\n          262,   649,   331,   967,   263,    11,  1509,   747,  1242,  1730,\n          331,  1158, 35833, 49663, 43185,   262,  2030, 45813,  3721, 10730,\n          284,   262,  1242,  1910,  1642,   465,  8440,  1664,  3288,   443,\n         2284,  2528,   260,   262,  4094, 18285,   379,   262,  2030, 45813,\n          351,  6143,  2272, 26399,   287,  6992,   286,  1160,  7319,   285,\n        31185,  1201,  2211,    13,  1509,   747, 19834,   331,  1158, 35833,\n        49663,    11, 17494,   262,   366,  2030, 45813,  5822, 33172,   318,\n          262,  3741, 15811,   287,   262,  1702, 11656,   290, 10632,   368,\n        24256,  2030,   538,  2096,   290,   468,   587,  2972,   306,  3417,\n          355,   262,  4870,   286,   262,  9779,  6862,  2030, 45813,    11,\n          393,   663,  4387, 29378,    11,   996,   287,   281,  2720,   287,\n        19318,  2023,  1584,   339,   531,   339,  6898,   691,   642,  4064,\n          286,   340,    11,   351,  7600,  4064,   286,   340,   852,  6898,\n          416,   262,  1509,   747,  1181,    13,   287,  2211,    11,   262,\n         2030, 45813,  2714,   546,   352,    13,   362,  1510,  2499,   286,\n         1242,    11,  7910,  1390,  1088,  8576,  2499,   416,   279, 18817,\n         8301, 28372,    13,   355,   880,   355,  1242,   290,  3869,  9210,\n           11,   262,  6841,  4909,   546,  1115,  1510, 14096,   286,  8237,\n           13,   287,  3717,    11,   262,   717, 15604,  2641,   262,  2030,\n        45813,   373,  4721,   416,   985,   261,   941,   263,    13,   584,\n        38462,  2291,   883,  1057,   416,  6450,   430,   664,   952,    13,\n          287,  2211,    11,   340,   373,  2098,   326,   257,   838,    11,\n        12877, 19862,   285,  7552,   561,  1280,   287,  1946,    13,   685,\n          325,    79,    60,   685, 15636,    60,   685, 15636,    60,   685,\n        15636,    60,   685, 15636,    60,   685, 15636,    60,   685, 15636,\n           60,   685, 15636,    60,   685, 15636,    60,   685, 15636,    60,\n          685, 15636,    60,   685, 15636,    60,   685, 15636,    60,   685,\n        15636,    60,   685, 15636,    60,   685, 15636,    60,   685, 15636,\n           60,   685, 15636,    60,   685, 15636,    60,   685, 15636,    60,\n          685, 15636,    60,   685, 15636,    60,   685, 15636,    60,   685,\n        15636,    60,   685, 15636,    60,   685, 15636,    60,   685, 15636,\n           60,   685, 15636,    60,   685, 15636,    60,   685, 15636,    60,\n          685, 15636,    60,   685, 15636,    60,   685, 15636,    60,   685,\n        15636,    60,   685, 15636,    60,   685, 15636,    60,   685, 15636,\n           60,   685, 15636,    60,   685, 15636,    60,   685, 15636,    60,\n          685, 15636,    60,   685, 15636,    60,   685, 15636,    60,   685,\n        15636,    60,   685, 15636,    60,   685, 15636,    60,   685, 15636,\n           60,   685, 15636,    60,   685, 15636,    60,   685, 15636,    60,\n          685, 15636,    60,   685, 15636,    60,   685, 15636,    60,   685,\n        15636,    60,   685, 15636,    60,   685, 15636,    60,   685, 15636,\n           60,   685, 15636,    60,   685, 15636,    60,   685, 15636,    60,\n          685, 15636,    60,   685, 15636,    60,   685, 15636,    60,   685,\n        15636,    60,   685, 15636,    60,   685, 15636,    60,   685, 15636,\n           60,   685, 15636,    60,   685, 15636,    60,   685, 15636,    60,\n          685, 15636,    60,   685, 15636,    60,   685, 15636,    60,   685,\n        15636,    60,   685, 15636,    60,   685, 15636,    60,   685, 15636,\n           60,   685, 15636,    60,   685, 15636,    60,   685, 15636,    60,\n          685, 15636,    60,   685, 15636,    60,   685, 15636,    60,   685,\n        15636,    60,   685, 15636,    60,   685, 15636,    60,   685, 15636,\n           60,   685, 15636,    60,   685, 15636,    60,   685, 15636,    60,\n          685, 15636,    60,   685, 15636,    60,   685, 15636,    60,   685,\n        15636,    60,   685, 15636,    60,   685, 15636,    60,   685, 15636,\n           60,   685, 15636,    60,   685, 15636,    60,   685, 15636,    60,\n          685, 15636,    60,   685, 15636,    60,   685, 15636,    60,   685,\n        15636,    60,   685, 15636,    60,   685, 15636,    60,   685, 15636,\n           60,   685, 15636,    60,   685, 15636,    60,   685, 15636,    60,\n          685, 15636,    60,   685, 15636,    60,   685, 15636,    60,   685,\n        15636,    60,   685, 15636,    60,   685, 15636,    60,   685, 15636,\n           60,   685, 15636,    60,   685, 15636,    60,   685, 15636,    60,\n          685, 15636,    60,   685, 15636,    60,   685, 15636,    60,   685,\n        15636,    60,   685, 15636,    60,   685, 15636,    60,   685, 15636,\n           60,   685, 15636,    60,   685, 15636,    60,   685, 15636,    60,\n          685, 15636,    60,   685, 15636,    60,   685, 15636,    60,   685,\n        15636,    60,   685, 15636,    60,   685, 15636,    60,   685, 15636,\n           60,   685, 15636,    60,   685, 15636,    60,   685, 15636,    60,\n          685, 15636,    60,   685, 15636,    60,   685, 15636,    60,   685,\n        15636,    60,   685, 15636,    60,   685, 15636,    60,   685, 15636,\n           60,   685, 15636,    60,   685, 15636,    60,   685, 15636,    60,\n          685, 15636,    60,   685, 15636,    60,   685, 15636,    60,   685,\n        15636,    60,   685, 15636,    60,   685, 15636,    60,   685, 15636,\n           60,   685, 15636,    60,   685, 15636,    60,   685, 15636,    60,\n          685, 15636,    60,   685, 15636,    60,   685, 15636,    60,   685,\n        15636,    60,   685, 15636,    60,   685, 15636,    60,   685, 15636,\n           60,   685, 15636,    60,   685, 15636,    60,   685, 15636,    60,\n          685, 15636,    60,   685, 15636,    60,   685, 15636,    60,   685,\n        15636,    60,   685, 15636,    60,   685, 15636,    60,   685, 15636,\n           60,   685, 15636,    60,   685, 15636,    60,   685, 15636,    60,\n          685, 15636,    60,   685, 15636,    60,   685, 15636,    60,   685,\n        15636,    60,   685, 15636,    60,   685, 15636,    60])"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "context_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[]"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "find_subsequences(context_encoded, answer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'the origins of the freeport can be traced back to 1888, but as it expanded in size, it adopted the “ opaque traditions of swiss banking ”, making it the preferred storage facility for the international elite. according to an article in the new yorker, swiss art deal yves bouvier pioneered the freeport concept parallel to the art market making his shipping company natural le coultre the biggest tenant at the freeport with storage space rented in excess of 20 thousand m² since 2013. swiss businessman yves bouvier, dubbed the \" freeport king \", is the majority investor in the singapore and luxembourg freeports and has been variously described as the owner of the geneva freeport, or its largest shareholder, though in an interview in october 2016 he said he owned only 5 % of it, with 85 % of it being owned by the swiss state. in 2013, the freeport held about 1. 2 million works of art, allegedly including around 1000 works by pablo picasso. as well as art and gold bars, the facility contains about three million bottles of wine. in 2009, the first gallery inside the freeport was opened by simon studer. other galleries include those run by sandra recio. in 2013, it was reported that a 10, 000 sq m extension would open in 2014. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "examples[0]['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}